+++
date = '2025-08-12T18:38:59+08:00'
draft = true
title = 'Writing Speed-of-Light Flash Attention for 5090 in CUDA C++'
url = 'fa-5090'
+++
In this post, I will walkthrough how I learned to implement Flash Attention for 5090 in CUDA C++. The main objective is to learn writing attention in CUDA C++, since many features are not available in [Triton](https://triton-lang.org/main/index.html), such as MXFP8 / NVFP4 MMA for sm120. I also feel this is a natural next step after learning about matmul kernels. Lastly, there are [many](https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html) [excellent](https://www.spatters.ca/mma-matmul) [blogposts](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog) on writing fast matmul kernels, but there is none for attention. So I want to take this chance to write up something nicely.

Readers are highly recommended to be familiar with CUDA C++ and how to use Tensor cores on NVIDIA GPUs. Of course you can still read along and clarify with your favourite LLMs along the way. Or you can check out GPU-MODE series ([slides](https://github.com/gpu-mode/lectures), [YouTube](https://www.youtube.com/@GPUMODE)) for basic CUDA C++ knowledge, as well as the excellent matmul blogposts mentioned above, to quickly get up to speed.

You can find the full implementation discussed in this post here: https://github.com/gau-nernst/learn-cuda/tree/7e2d6951c3fb2b0211dca756fb2144126a352013/07_attention. For `bs=1, num_heads=8, len_query=4096, len_kv = 8192`, 5090 @ 400W, compile with CUDA 12.9, I obtained the following benchmark results (theoretical limit of 5090 is 209.5 TFLOPS for BF16)

Kernel                         | TFLOPS | % of SOL
-------------------------------|--------|---------
`F.sdpa()` (Flash Attention)   | 186.73 | 89.13%
`F.sdpa()` (CuDNN)             | 203.61 | 97.19%
`flash-attn`                   | 190.58 | 90.97%
v1 (basic)                     | 142.87 | 68.20%
v2 (shared memory swizzling)   | 181.11 | 86.45%
v3 (2-stage pipelining)        | 189.84 | 90.62%
v4 (`ldmatrix.x4` for K and V) | 194.33 | 92.76%
v5 (better pipelining)         | 197.74 | 94.39%

Do note that although I only use Ampere features in these implementations (sm120 supports `cp.async.bulk` i.e. TMA, but I don't use it here), my implementations might not run performantly on earlier generations of GPUs. Due to improvements in newer hardware, you might need to use more tricks to reach Speed-of-Light on older GPUs e.g. pipeline shared memory to register memory data movements.

## Flash Attention algorithm

Let's start with the reference implementation of attention.

```python
from torch import Tensor

def sdpa(q: Tensor, k: Tensor, v: Tensor):
    # q: [B, Lq, DIM]
    # k: [B, Lk, DIM]
    # v: [B, Lk, DIM]
    D = q.shape[-1]
    scale = D ** -0.5
    attn = (q @ k.transpose(-1, -2)) * scale  # [B, Lq, Lk]
    attn = attn.softmax(dim=-1)
    out = attn @ v  # [B, Lq, DIM]
    return out
```

Technically, if the inputs are BF16, some computations should remain in FP32, especially softmax. However, for brevity, we omit them.

We are implementing the algorithm outlined in the [Flash Attention 2 paper](https://arxiv.org/abs/2307.08691)

- `tile_Q` stays in registers throughout -> `DIM` must be small -> 128.
- Don't need to care about online softmax for now.

```python
scale = DIM ** -0.5
tile_O = torch.zeros(BLOCK_Q, DIM)
tile_Q = load_tile_Q(...)  # [BLOCK_Q, DIM]

for tile_KV_idx in range(Lk // BLOCK_KV):
    # first MMA: S = Q @ K.T
    # (BLOCK_Q, DIM) x (BLOCK_KV, DIM).T -> (BLOCK_Q, BLOCK_KV)
    tile_Q                             # (BLOCK_Q, DIM)
    tile_K = load_tile_K(tile_KV_idx)  # (BLOCK_KV, DIM)
    tile_S = tile_Q @ tile_K.T         # (BLOCK_Q, BLOCK_KV)
    tile_S = tile_S * scale

    # online softmax and rescale tile_O
    ...

    # second MMA: O = P @ V
    # (BLOCK_Q, BLOCK_KV) x (BLOCK_KV, DIM) -> (BLOCK_Q, DIM)
    tile_P                             # (BLOCK_Q, BLOCK_KV)
    tile_V = load_tile_V(tile_KV_idx)  # (BLOCK_KV, DIM)
    tile_O += tile_P @ tile_V          # (BLOCK_Q, DIM)

# write output tile
...
```

## Version 1 - Basic implementation

We will follow the typical MMA flow
- Load a 2D tile of data from global memory to shared memory using [`cp.async`](https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async). This requires Ampere (sm80 and above).
- Load data from shared memory to register memory using [`ldmatrix`](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions-ldmatrix).
- Call [`mma.m16n8k16`](https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float) for BF16 matrix multiplication (and accumulate).

Since this is not the focus of this blogpost, I will go through these briefly. Readers are welcome to refer to [xxx]() for more detailed explanation.

### Hierarchical tiling

- Threadblock
- Warp
- MMA

### Global to Shared memory data transfer

The following templated function will do a 2D tile copy from global memory to shared memory.
- Shape of the 2D tile is specified via `HEIGHT` and `WIDTH`.
- `dst` is shared memory address, `src` is global memory address.
- Global memory `src` is row-major, so `src_stride` specifies how much to move to the next row.
- Shared memory `dst` is also row-major, and will be stored as a contiguous block -> `dst_stride` is `WIDTH`.

```cpp
#include <cuda_bf16.h>

template <int HEIGHT, int WIDTH, int TB_SIZE>
__device__ inline
void global_to_shared(uint32_t dst, const nv_bfloat16 *src, int src_stride, int tid) {
  constexpr int num_elems = 16 / sizeof(nv_bfloat16);
  constexpr int num_iters = HEIGHT * WIDTH / (TB_SIZE * num_elems);

  for (int iter = 0; iter < num_iters; iter++) {
    const int idx = (iter * TB_SIZE + tid) * num_elems;
    const int row = idx / WIDTH;
    const int col = idx % WIDTH;

    const uint32_t dst_addr = dst + (row * WIDTH + col) * sizeof(nv_bfloat16);
    const nv_bfloat16 *src_addr = src + (row * src_stride + col);
    asm volatile("cp.async.cg.shared.global [%0], [%1], 16;" :: "r"(dst_addr), "l"(src_addr));
  }
}
```

We will use inline assembly to write `cp.async.cg.shared.global`. This PTX will do 16-byte transfer, or 8 BF16 elements (`num_elems = 16 / sizeof(nv_bfloat16)`), for each CUDA thread. To ensure coalesced memory access, consecutive threads will be responsible for consecutive 8xBF16 groups.

Note:
- The loop `for (int iter = 0; iter < num_iters; iter++)` is written this way so that the compiler (`nvcc`) can fully unroll the loop. `num_iters` is known at compile time (guaranteed by `constexpr`). If we mix `tid` in the loop, which is a "dynamic" variable to the compiler, the loop can't be unrolled, even when we know certain constraints about the variable i.e. `tid < TB_SIZE`.

### Shared memory to Register memory data transfer

Since we are using `mma.m16n8k16` instruction, each MMA 16x8 output tile (`m16n8`) requires 16x16 A tile (`m16k16`) and 8x16 B tile (`n8k16`). `ldmatrix` can load one, two, or four 8x8 tile(s) of 16-bit elements. Hence,
- A tile `m16k16` requires four 8x8 tiles -> `ldmatrix.x4`
- B tile `n8k16` requires two 8x8 tiles -> `ldmatrix.x2`

### Kernel

```cpp
constexpr int DIM = 128;

__global__

```

### Online softmax

### Benchmark setup

Wow, that's plentiful for the 1st version. Indeed, I spent the most time on version 1 trying to implement Flash Attention correctly. Took me 2 days to realize [`__shfl_xor_sync()`'s mask should be 2 (`0b10`) instead of `0x10` for butterfly reduction](https://github.com/gau-nernst/learn-cuda/commit/8fdb3e6a95a18502c2250571eeeb2179860936c0).

That's fine, we still have a few tricks up our sleeves for the next few versions

## Version 2 - Shared memory swizzling

Nsight. Stall short scoreboard -> shared memory. ...

NVIDIA's shared memory is backed by 32 memory banks. Consecutive 4-byte memory addresses are assigned to consecutive memory banks. This poses a problem when we load data from shared memory to register memory with `ldmatrix` - 

## Version 3 - 2-stage pipelining

## Version 4 - `ldmatrix.x4` for K and V

## Version 5 - better pipelining
